{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6e1d2-cfd3-4b3b-9c6c-a23b30244507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how we can use the wisper-large-v3 model for the audio to text conversion\n",
    "# Note: thise cell is just instruction that you have to following in following code snippets \n",
    "1. go to the groq \n",
    "2. select the model\n",
    "3. generate the api\n",
    "4. use that api \"past_api_here\" in following code \n",
    "5. download the audio if you have that greate \n",
    "6. past the \"file_path\" of your audio \n",
    "7. run that code cell and you will get the all the text that in your audio file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04fff2e3-4890-459f-a208-7042cb4c8b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How much money do you have in your account? Which one? Your main one. I can't see my main bank account. That's... Oh, impressive. You should just count how many numbers there are. 1, 2, 3, 4, 5, 6, 7, 8. Yeah.\n"
     ]
    }
   ],
   "source": [
    "# in following code that is the example for the small audio file we are testing \n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set your API key here\n",
    "API_KEY = \"past_api_here\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize the client with the API key\n",
    "client = Groq(api_key=API_KEY)\n",
    "\n",
    "# Use os.path.join to ensure proper path handling\n",
    "filename = os.path.join(r\"file_path\")\n",
    "\n",
    "# Open the file and send it for transcription\n",
    "with open(filename, \"rb\") as file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=(filename, file.read()),\n",
    "        model=\"whisper-large-v3\",\n",
    "        response_format=\"verbose_json\",\n",
    "    )\n",
    "    print(transcription.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "893a369e-937c-438f-b893-39d702029e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey YouTube, in this video I'm going to show you how you can quickly convert any audio into text using the free open source package in Python called Whisper. I'm going to show I installed it, show an example of how I ran it, and compare it to an existing library. So starting off, you'll probably want to go to the Whisper GitHub repository that we're looking at here, and they give instructions on how you can install it. Now one thing to keep in mind when you pip install just the name whisper it's not going to install the right version. We want to install from this git repository so just take this pip install command and run it in your environment that you're running python. And they also mention here that you need ffmpeg installed. There's some instructions to do it but I already had that installed on my computer. Now that I have whisper installed let's just make some audio that I can test this on. So I'm going I'm gonna say some idioms. Idioms are usually hard for models to understand even though this is just speech to text. This will be kind of fun. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly. I be like a fish out of water and as fit as a fiddle to be under the weather Let save this off I gonna save it as a wave They do have instructions for how we could run this just straight from the command line once it installed I going to show you how to use the Python API which they show here So it really simple. We just import whisper. Then we're going to create our model, which is we're going to load the model that's called base. And then just using this model object, we run transcribe on our audio file so I named it idioms let's use the wave version we want this to return the result now I noticed when I ran this before I get this error because of CUDA's half tensor and float tensor was able to solve this so that's something to keep in mind if it doesn't work for you you might need to set floating point 16 to fault and you can see after it's run here it detected the language already as English. And then this result object has a few different methods in them. But what we want to get inside of this is just the text. We could see that it looks like the result is good. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly. I'd be like a fish out of water in this It did mess up a little bit this fish out of water in as fit as a fiddle Maybe I didn say it clearly enough Another thing to know is when you first run this it gonna have to download the base model So you might see a progress bar going across and you'll have to download that model. And it says when you run this transcribe, it's actually taking 30 second chunks of your audio file and running predictions on it. Now there's also another approach that you can take, which is a lower level approach where you actually create the model and then you create the audio object and pad or trim this. What this will do is just make sure that this audio chunk is only 30 seconds or it'll pad it with 30 seconds since that's the length the model expects to have as input. Then it's making a log MEL spectrogram. It's detecting the language and we can decode here and provide a lot more options if we wanted to. If I run this cell again get this error which I now can set in the decoding options, FP16 equals false. And actually this time it looks like it got everything correct. I'd be like a fish out of water and as fit as a fiddle. So that's it for Whisper. I just want to compare it to an existing type of model. And a popular library for doing this is the speech recognition library The way we run the speech recognition library is we import it and then create this recognizer object which we then can load our audio file with After that you could take the recognizer object and there are a few different recognizing methods for that and we're going to use the google recognize and let's see what the result is. So it looks like it didn't add any punctuation and the cloud nine is different. I would love to be on cloud nine as a one-trick pony that wouldn't hurt a fly. But the one thing to keep in mind is that this is actually using the Google Speech Recognition API. The Whisper library, you actually have the model downloaded and it's yours to use. I do also recommend you take a look at the Whisper paper, which was released with this code. They also go into detail about how the model was trained and the architecture that it's used. Whisper does work on a bunch of different languages. The performance, they say, varies based on the language. So you can go here on the GitHub repo where they have a plot showing which languages actually performs best for the bars here. Smaller is better and larger means it performs worse. So still pretty impressive the number of languages that this model works on.\n"
     ]
    }
   ],
   "source": [
    "# in following code that is the example for the Larger audio file we are testing \n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set your API key here\n",
    "API_KEY = \"past_api_here\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize the client with the API key\n",
    "client = Groq(api_key=API_KEY)\n",
    "\n",
    "# Use os.path.join to ensure proper path handling\n",
    "filename = os.path.join(r\"file_path\")\n",
    "\n",
    "# Open the file and send it for transcription\n",
    "with open(filename, \"rb\") as file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=(filename, file.read()),\n",
    "        model=\"whisper-large-v3\",\n",
    "        response_format=\"verbose_json\",\n",
    "    )\n",
    "    print(transcription.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d60a477-5e88-486b-b0ca-3603da40afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4853\n"
     ]
    }
   ],
   "source": [
    "print(len(transcription.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98278792-e33b-4970-8916-bc17805a6b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " चलिए सबसे पहले बात करते हैं podcast के बारे में कि आखिर ये podcast होता क्या है जो ये इतनी popular term आज के टाइम में हो गई है तो देखिए अगर जैसे सुना हो सकता है आपने recently ही इस term को हो लेकिन ये बहुत time पहले से जल रही है क्योंकि ये जो term थी ये सबसे पहले जो launch हुई थी वो हुई थी radio के टाइम में तो मान लीजिए दो लोग किसी एक particular topic पे बात कर रहे हैं आप topic पर बात करते करते कई बार ये हो जाता था कि एक episode में उस topic के बारे में सारी बात नहीं खतम हो पाती थी तो फिर वो धीरे धीरे क्या करने लगे episodes बनाने लगे कि episodes 1 episode 2, episode 3 तो वो क्या बन गई एक podcast की series बन गई normally अगर मैं podcast की बात करूँ तो podcast is a series या फिर a collection of audio files ठीक है यहाँ पर बात हो रही है audio files की जिसे या तो आप internet से download कर सकते हो जिसे या तो आप radio पर सुन सकते हो या फिर generally अभी जिसे YouTube पर आ गया तो यूटिव पर भी हम सुन सकते हैं\n"
     ]
    }
   ],
   "source": [
    "# we checking the hindi audio not that is proved it supports the multi language \n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set your API key here\n",
    "API_KEY = \"past_api_key\"  \n",
    "\n",
    "# Initialize the client with the API key\n",
    "client = Groq(api_key=API_KEY)\n",
    "\n",
    "# Use os.path.join to ensure proper path handling\n",
    "filename = os.path.join(r\"file_path\")\n",
    "\n",
    "# Open the file and send it for transcription\n",
    "with open(filename, \"rb\") as file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=(filename, file.read()),\n",
    "        model=\"whisper-large-v3\",\n",
    "        response_format=\"verbose_json\",\n",
    "    )\n",
    "    print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4239dd7a-8e47-419a-8b93-94636b6db3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n"
     ]
    }
   ],
   "source": [
    "print(len(transcription.text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91a78d2c-1a23-45e8-910f-f153289a81b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
